{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and Lookup (TF Hub and Annoy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This example logs a `class` (instead of an object instance) as a model.\n",
    "This allows for custom setup configuration in the class's `__init__()` method,  \n",
    "and access to logged artifacts at deployment time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import verta\n",
    "except ImportError:\n",
    "    !pip install verta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example features:\n",
    "- embedding model built around a [**TensorFlow Hub**](https://www.tensorflow.org/hub/) module\n",
    "- nearest neighbor embedding lookup via [**Annoy**](https://github.com/spotify/annoy)\n",
    "- **verta**'s Python client logging a `class` as a model to be instantiated at deployment time\n",
    "- predictions against a deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"app.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"Film Review Embeddings\"\n",
    "EXPERIMENT_NAME = \"TF Hub and Annoy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['VERTA_EMAIL'] = \n",
    "# os.environ['VERTA_DEV_KEY'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import wget\n",
    "except ImportError:\n",
    "    !pip install wget  # you may need pip3\n",
    "    import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_url = \"http://s3.amazonaws.com/verta-starter/imdb_master.csv\"\n",
    "train_data_filename = wget.detect_filename(train_data_url)\n",
    "if not os.path.isfile(train_data_filename):\n",
    "    wget.download(train_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_reviews = pd.read_csv(train_data_filename, encoding='latin')['review'].values.tolist()\n",
    "reviews = all_reviews[:2000]  # just a subset for this example\n",
    "\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)\n",
    "run = client.set_experiment_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Nearest Neighbor Embedding Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_LENGTH = 512\n",
    "NN_INDEX_FILENAME = \"reviews.ann\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"tf_cache_dir\"\n",
    "\n",
    "# define graph\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "    encoder = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "    embed = encoder(text_input)\n",
    "    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "g.finalize()\n",
    "\n",
    "# initialize session\n",
    "sess = tf.Session(graph=g)\n",
    "sess.run(init_op)\n",
    "\n",
    "# build and save embedding index\n",
    "t = annoy.AnnoyIndex(EMBEDDING_LENGTH, 'angular')  # Length of item vector that will be indexed\n",
    "for i, review in enumerate(reviews):\n",
    "    # produce embedding with TF\n",
    "    embedding = sess.run(embed, feed_dict={text_input: [review]})\n",
    "    t.add_item(i, embedding[0])\n",
    "t.build(10) # 10 trees\n",
    "t.save(NN_INDEX_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_artifact(\"nn_index\", open(NN_INDEX_FILENAME, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TensorFlow model—particularly one using TensorFlow Hub and a pre-built Annoy index—will require some setup at deployment time.\n",
    "\n",
    "To support this, the Verta platform allows a model to be defined as a `class` that will be instantiated when it's deployed.  \n",
    "This class should have provide the following interface:\n",
    "\n",
    "- `__init__(self, artifacts)` where `artifacts` is a mapping of artifact keys to filepaths. This will be explained below, but Verta will provide this so you can open these artifact files and set up your model. Other initialization steps would be in this method, as well.\n",
    "- `predict(self, data)` where `data`—like in other custom Verta models—is a list of input values for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingAndLookupModel:\n",
    "    def __init__(self, artifacts):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        artifacts\n",
    "            Mapping of Experiment Run artifact keys to filepaths.\n",
    "            This is provided by ``run.fetch_artifacts(artifact_keys)``.\n",
    "        \n",
    "        \"\"\"\n",
    "        # get artifact filepath from `artifacts` mapping\n",
    "        annoy_index_filepath = artifacts['nn_index']\n",
    "        \n",
    "        # load embedding index\n",
    "        self.index = annoy.AnnoyIndex(EMBEDDING_LENGTH, \"angular\")\n",
    "        self.index.load(annoy_index_filepath)\n",
    "        \n",
    "        os.environ[\"TFHUB_CACHE_DIR\"] = \"tf_cache_dir\"\n",
    "        \n",
    "        # define graph\n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            self.text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "            self.encoder = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "            self.embed = self.encoder(self.text_input)\n",
    "            init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        g.finalize()\n",
    "        self.graph = g\n",
    "        \n",
    "        # initialize session\n",
    "        self.session = tf.Session(graph=self.graph)\n",
    "        self.session.run(init_op)\n",
    "     \n",
    "    def predict(self, data):\n",
    "        predictions = []\n",
    "        for review in data:\n",
    "            # embed sentence\n",
    "            embedding = self.session.run(self.embed, feed_dict={self.text_input: [review]})\n",
    "\n",
    "            # find closest\n",
    "            predictions.append({\n",
    "                review: self.index.get_nns_by_vector(embedding[0], 10)\n",
    "            })\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we logged an artifact with the key `\"nn_index\"`.  \n",
    "You can obtain an `artifacts` mapping mentioned above using `run.fetch_artifacts(keys)` to work with locally.  \n",
    "A similar mapping—that works identically—will be passed into `__init__()` when the model is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = run.fetch_artifacts([\"nn_index\"])\n",
    "\n",
    "model = EmbeddingAndLookupModel(artifacts=artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([\"Good film.\", \"Bad film!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys expected in the `artifacts` mapping mentioned above must be passed into `run.log_model()` to be available during deployment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run.log_model(\n",
    "    model=EmbeddingAndLookupModel,\n",
    "    artifacts=['nn_index'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to make sure we provide every package involved in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_requirements([\n",
    "    \"annoy==1.16.2\",\n",
    "    \"tensorflow\",\n",
    "    \"tensorflow_hub\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Live Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the Experiment Run through the Verta Web App and deploy it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = all_reviews[-2000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Deployed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta.deployment import DeployedModel\n",
    "\n",
    "deployed_model = DeployedModel(HOST, run.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Deployed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for review in reviews:\n",
    "    print(deployed_model.predict([review]))\n",
    "    time.sleep(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
