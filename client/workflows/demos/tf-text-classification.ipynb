{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackOverflow Question Multiclassification (Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses a dataset from stackoverflow, with the input being posts and predictions one of the possible classes. It's an adapted version from the Keras example available at https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568.\n",
    "\n",
    "It shows how to use Keras for training a tokenizer and a model. It also shows how to build a custom model for deployment of this hybrid model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been tested with the following packages:  \n",
    "(you may need to change `pip` to `pip3`, depending on your own Python environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Python 3.6\n",
    "!pip install verta\n",
    "!pip install wget\n",
    "!pip install pandas\n",
    "!pip install tensorflow==1.14.0\n",
    "!pip install scikit-learn\n",
    "!pip install lxml\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Verta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = 'app.verta.ai'\n",
    "\n",
    "PROJECT_NAME = 'Text Classification'\n",
    "EXPERIMENT_NAME = 'basic-clf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['VERTA_EMAIL'] = \n",
    "# os.environ['VERTA_DEV_KEY'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST, use_git=False)\n",
    "\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)\n",
    "run = client.set_experiment_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ew7HTbPpCJH",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import wget\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset, load and reduce the number of examples so that the example runs faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('stack-overflow-data.csv'):\n",
    "    wget.download('https://storage.googleapis.com/tensorflow-workshop-examples/stack-overflow-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('stack-overflow-data.csv')\n",
    "df = df[pd.notnull(df['tags'])]\n",
    "df = df[:5000]\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process the data removing unnecessary characters so that we have only the main text of the post left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    return text\n",
    "    \n",
    "df['post'] = df['post'].apply(clean_text)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the dataset into the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(df) * .7)\n",
    "train_posts = df['post'][:train_size]\n",
    "train_tags = df['tags'][:train_size]\n",
    "\n",
    "test_posts = df['post'][train_size:]\n",
    "test_tags = df['tags'][train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use keras for tokenization, with a trained tokenizer on the given corpus. This will learn what words to use based on their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "tokenize = keras.preprocessing.text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, transform the text input into a numeric array and encode the labels as one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "run.log_attribute('classes', encoder.classes_.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and train the numeric Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'hidden_size': 512,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 1024,\n",
    "    'num_epochs': 2,\n",
    "    'optimizer': \"adam\",\n",
    "    'loss': \"categorical_crossentropy\",\n",
    "    'validation_split': 0.1,\n",
    "}\n",
    "run.log_hyperparameters(hyperparams)\n",
    "\n",
    "# Build the model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(hyperparams['hidden_size'], input_shape=(max_words,)))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout(hyperparams['dropout']))\n",
    "model.add(keras.layers.Dense(num_classes))\n",
    "model.add(keras.layers.Activation('softmax'))\n",
    "\n",
    "model.compile(loss=hyperparams['loss'],\n",
    "              optimizer=hyperparams['optimizer'],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# create a per-epoch callback for logging\n",
    "def log_validation_callback(epoch, logs):  # Keras will call this each epoch\n",
    "    run.log_observation(\"train_loss\", float(logs['loss']))\n",
    "    run.log_observation(\"train_acc\", float(logs['acc']))\n",
    "    run.log_observation(\"val_loss\", float(logs['val_loss']))\n",
    "    run.log_observation(\"val_acc\", float(logs['val_acc']))\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=hyperparams['batch_size'],\n",
    "                    epochs=hyperparams['num_epochs'],\n",
    "                    verbose=1,\n",
    "                    validation_split=hyperparams['validation_split'],\n",
    "                    callbacks=[keras.callbacks.LambdaCallback(on_epoch_end=log_validation_callback)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = model.evaluate(x_train, y_train,\n",
    "                                       batch_size=hyperparams['batch_size'], verbose=1)\n",
    "run.log_metric(\"train_loss\", train_loss)\n",
    "run.log_metric(\"train_acc\", train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the requirements for model prediction time, then create the model API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXXx5Oc3pOmN"
   },
   "outputs": [],
   "source": [
    "import six, tensorflow, bs4, sklearn\n",
    "requirements = [\n",
    "    \"numpy\",\n",
    "    \"tensorflow\",\n",
    "    \"beautifulsoup4\",\n",
    "    \"scikit-learn\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_api = ModelAPI([\"blah\", \"blah\", \"blah\"], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the model API looks like what we are expecting: a string as input and multiple numbers are the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_api.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the model wrapper for prediction. It's more complicated than a regular wrapper because Keras models can't be serialized with the rest of the class, so we have to save them as hdf5 and load once at prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper:\n",
    "    def __init__(self, keras_model, tokenizer):\n",
    "        # save Keras model\n",
    "        import six  # this comes installed with Verta\n",
    "        self.keras_model_hdf5 = six.BytesIO()\n",
    "        keras_model.save(self.keras_model_hdf5)\n",
    "        self.keras_model_hdf5.seek(0)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        import tensorflow\n",
    "\n",
    "        # restore instance attributes\n",
    "        self.__dict__.update(state)\n",
    "\n",
    "        # load Keras model\n",
    "        self.graph = tensorflow.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.session = tensorflow.Session()\n",
    "            with self.session.as_default():\n",
    "                self.keras_model = tensorflow.keras.models.load_model(state['keras_model_hdf5'])\n",
    "\n",
    "    def predict(self, data):\n",
    "        import numpy, tensorflow\n",
    "        tokenized_input = self.tokenizer.texts_to_matrix(data)\n",
    "        \n",
    "        if hasattr(self, 'keras_model'):\n",
    "            with self.session.as_default():\n",
    "                with self.graph.as_default():\n",
    "                    return self.keras_model.predict(tokenized_input)\n",
    "        else:  # not unpickled\n",
    "            model = tensorflow.keras.models.load_model(self.keras_model_hdf5)\n",
    "            return model.predict(tokenized_input)\n",
    "\n",
    "\n",
    "model_wrapper = ModelWrapper(model, tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the predict method behaves as we'd expect, since it will be called by the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper.predict([\"foo bar baz\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the model information necessary for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_model(model_wrapper, model_api=model_api)\n",
    "run.log_requirements(requirements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the demo library to query the model one example at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta._demo_utils import DeployedModel\n",
    "\n",
    "deployed_model = DeployedModel(HOST, run.id)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the model through the Web App, then make predictions through the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, time\n",
    "for x in itertools.cycle(test_posts.tolist()):\n",
    "    print(deployed_model.predict([x]))\n",
    "    time.sleep(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "basic-text-classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
