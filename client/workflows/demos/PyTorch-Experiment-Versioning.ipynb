{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single FC-NN (PyTorch) Experiment Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/VertaAI/modeldb/blob/master/client/workflows/demos/PyTorch-Experiment-Versioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is based on our [basic PyTorch example](../examples/pytorch.ipynb).\n",
    "\n",
    "The example features:\n",
    "- Single FC-NN (PyTorch) model \n",
    "- Experiment tracking and versioning with Verta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Verta Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart your notebook if prompted on Colab\n",
    "try:\n",
    "    import verta\n",
    "except ImportError:\n",
    "    !pip install verta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"app.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"Census Income Classification\"\n",
    "EXPERIMENT_NAME = \"Single FC-NN\"\n",
    "WORKSPACE = \"XXXXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['VERTA_EMAIL'] = 'XXXXXXXXXXXX'\n",
    "os.environ['VERTA_DEV_KEY'] = 'XXXXXXXXXXXXXXXXXXXXXXXX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import wget\n",
    "except ImportError:\n",
    "    !pip install wget  # you may need pip3\n",
    "    import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section demonstrates logging model metadata and training artifacts to ModelDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta import Client\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME, workspace=WORKSPACE, public_within_org=True)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta.dataset import S3\n",
    "\n",
    "dataset = client.set_dataset(name=\"Census Income S3\")\n",
    "dataset_version = dataset.create_version(S3(\"s3://verta-starter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_url = \"http://s3.amazonaws.com/verta-starter/census-train.csv\"\n",
    "train_data_filename = wget.detect_filename(train_data_url)\n",
    "if not os.path.isfile(train_data_filename):\n",
    "    wget.download(train_data_url)\n",
    "\n",
    "test_data_url = \"http://s3.amazonaws.com/verta-starter/census-test.csv\"\n",
    "test_data_filename = wget.detect_filename(test_data_url)\n",
    "if not os.path.isfile(test_data_filename):\n",
    "    wget.download(test_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_data_filename)\n",
    "X = df_train.iloc[:,:-1]\n",
    "y = df_train.iloc[:, -1]\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather indices to split training data into training and validation sets\n",
    "shuffled_idxs = np.random.permutation(len(y))\n",
    "idxs_train = shuffled_idxs[int(len(shuffled_idxs)/10):]  # last 90%\n",
    "idxs_val = shuffled_idxs[:int(len(shuffled_idxs)/10)]  # first 10%\n",
    "\n",
    "X_train, y_train = (torch.tensor(X.values[idxs_train], dtype=torch.float),\n",
    "                    torch.tensor(y.values[idxs_train], dtype=torch.long))\n",
    "X_val, y_val = (torch.tensor(X.values[idxs_val], dtype=torch.float),\n",
    "                torch.tensor(y.values[idxs_val], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataset object to support batch training\n",
    "class TrainingDataset(data_utils.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.features[idx], self.labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc      = nn.Linear(num_features, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output  = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)  # flatten non-batch dimensions\n",
    "        x = func.relu(self.fc(x))\n",
    "        x = self.dropout(x)\n",
    "        x = func.softmax(self.output(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_candidates = {\n",
    "    \"hidden_size\" : [256, 512],\n",
    "    \"dropout\" : [0.2, 0.3],\n",
    "    \"loss_fn\" : [\"cross_entropy\"],\n",
    "    \"optimizer\" : [\"adam\"],\n",
    "    \"num_epochs\" : [5, 10], \n",
    "    \"batch_size\" : [128, 256]\n",
    "}\n",
    "hyperparam_sets = [dict(zip(hyperparam_candidates.keys(), values))\n",
    "                   for values\n",
    "                   in itertools.product(*hyperparam_candidates.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta.environment import Python\n",
    "\n",
    "def train_model(hyperparam_set):\n",
    "    run = client.set_experiment_run()\n",
    "    \n",
    "    run.log_attributes({\n",
    "        'library': \"pytorch\",\n",
    "        'architecture': \"fully-connected\",\n",
    "    })\n",
    "    \n",
    "    # log git information\n",
    "    run.log_code()\n",
    "    \n",
    "    # create model and training optimizer\n",
    "    run.log_hyperparameters(hyperparam_set)\n",
    "    model = Net(\n",
    "        num_features=X.shape[1],\n",
    "        hidden_size=hyperparam_set[\"hidden_size\"],\n",
    "        dropout=hyperparam_set[\"dropout\"],\n",
    "    )\n",
    "    if hyperparam_set[\"loss_fn\"] == \"cross_entropy\":   \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    if hyperparam_set[\"optimizer\"] == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    # enable batching of training data\n",
    "    dataloader = data_utils.DataLoader(\n",
    "        TrainingDataset(X_train, y_train),\n",
    "        batch_size=hyperparam_set[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    run.log_dataset_version(\"census_data\", dataset_version)  # log dataset metadata\n",
    "    run.log_training_data(X, y)  # log histogram of training data\n",
    "    \n",
    "    for i_epoch in range(hyperparam_set[\"num_epochs\"]):\n",
    "        for i_batch, (X_batch, y_batch) in enumerate(dataloader):\n",
    "            model.zero_grad()  # reset model gradients\n",
    "            \n",
    "            output = model(X_batch)  # conduct forward pass\n",
    "            \n",
    "            loss = criterion(output, y_batch)  # compare model output w/ ground truth\n",
    "            \n",
    "            print(\n",
    "                \"\\repoch {}/{} | iteration {}/{} | epoch loss avg: {}\"\n",
    "                .format(i_epoch+1, hyperparam_set[\"num_epochs\"], i_batch+1, len(dataloader), loss.item()),\n",
    "                end=''\n",
    "            )\n",
    "            \n",
    "            loss.backward()  # backpropogate loss to calculate gradients\n",
    "            optimizer.step()  # update model weights\n",
    "            \n",
    "        with torch.no_grad():  # no need to calculate gradients when assessing accuracy\n",
    "            print()\n",
    "            \n",
    "            pred_train = model(X_train).numpy().argmax(axis=1)\n",
    "            train_acc = (pred_train == y_train.numpy()).mean()\n",
    "            print(\"Training accuracy: {}\".format(train_acc))\n",
    "            run.log_observation(\"train_acc\", train_acc)\n",
    "\n",
    "            pred_val = model(X_val).numpy().argmax(axis=1)\n",
    "            val_acc = (pred_val == y_val.numpy()).mean()\n",
    "            print(\"Validation accuracy: {}\".format(val_acc))\n",
    "            run.log_observation(\"val_acc\", val_acc)\n",
    "            \n",
    "            run.log_artifact(\"epoch_{}_checkpoint\".format(i_epoch), model)\n",
    "    \n",
    "    with torch.no_grad():  # no need to calculate gradients when assessing accuracy\n",
    "        pred_train = model(X_train).numpy().argmax(axis=1)\n",
    "        train_acc = (pred_train == y_train.numpy()).mean()\n",
    "        print(\"Training accuracy: {}\".format(train_acc))\n",
    "    run.log_metric(\"train_acc\", train_acc)\n",
    "    run.log_model(model)\n",
    "    run.log_requirements([\"torch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for hyperparams in hyperparam_sets:\n",
    "    train_model(hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Staging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the best run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = proj.expt_runs.sort(\"metrics.train_acc\", descending=True)[0]\n",
    "print(\"Training Accuracy: {:.4f}\".format(best_run.get_metric(\"train_acc\")))\n",
    "\n",
    "best_hyperparams = best_run.get_hyperparameters()\n",
    "print(\"Hyperparameters: {}\".format(best_hyperparams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing model can be staged as a *registered model*, for use downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTERED_MODEL_NAME = \"Fully-Connected Census Classifier\"\n",
    "MODEL_VERSION_NAME = \"v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model = client.get_or_create_registered_model(\n",
    "    name=REGISTERED_MODEL_NAME,\n",
    "    workspace=WORKSPACE, public_within_org=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model.create_version_from_run(best_run.id, name=MODEL_VERSION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This registered model version can be deployed to an endpoint, whereupon predictions can be made via a REST endpoint or through the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model = client.get_registered_model(name=REGISTERED_MODEL_NAME, workspace=WORKSPACE)\n",
    "model_version = registered_model.get_version(name=MODEL_VERSION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and update an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = client.get_or_create_endpoint(path=\"/census\", workspace=WORKSPACE, public_within_org=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "endpoint.update(model_version, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare \"live\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_data_filename)\n",
    "X_test = df_test.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model = endpoint.get_deployed_model()\n",
    "\n",
    "for x in itertools.cycle(X_test.sample(frac=1).values.tolist()):\n",
    "    print(np.around(deployed_model.predict([x]), decimals=8))\n",
    "    time.sleep(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.delete()\n",
    "registered_model.delete()\n",
    "proj.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
