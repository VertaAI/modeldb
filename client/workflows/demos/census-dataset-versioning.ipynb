{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Grid Search (scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/VertaAI/modeldb/blob/master/client/workflows/demos/census-dataset-versioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart your notebook if prompted on Colab\n",
    "try:\n",
    "    import verta\n",
    "except ImportError:\n",
    "    !pip install verta\n",
    "    import verta\n",
    "\n",
    "print(\"Using Verta version\", verta.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"XXXXX.app.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"Census Income Classification\"\n",
    "EXPERIMENT_NAME = \"Logistic Regression\"\n",
    "WORKSPACE = \"XXXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['VERTA_EMAIL'] = 'XXXXXX@XXXX.XXX'\n",
    "os.environ['VERTA_DEV_KEY'] = 'XXXXXXXXXXXXXXXXXXXXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of creating an AWS credentials file to enable S3 dataset versioning on Colab.\n",
    "# It is NOT RECOMMENDED to store secrets, such as AWS or Verta credentials, in code.\n",
    "import os\n",
    "\n",
    "AWS_ACCESS_KEY_ID = \"XXXXX\"\n",
    "AWS_SECRET_ACCESS_KEY = \"XXXXX\"\n",
    "\n",
    "aws_config_dir = os.path.expanduser(\"~/.aws\")\n",
    "if not os.path.exists(aws_config_dir):\n",
    "    os.makedirs(aws_config_dir)\n",
    "\n",
    "aws_credentials_filepath = os.path.join(aws_config_dir, \"credentials\")\n",
    "if not os.path.exists(aws_credentials_filepath):\n",
    "    with open(aws_credentials_filepath, 'w') as f:\n",
    "        f.write('\\n'.join([\n",
    "            \"[default]\",\n",
    "            \"aws_access_key_id={}\".format(AWS_ACCESS_KEY_ID),\n",
    "            \"aws_secret_access_key={}\".format(AWS_SECRET_ACCESS_KEY),\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install additional packages\n",
    "# NOTE: you may need pip3 instead of pip\n",
    "\n",
    "!pip install boto3\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "import boto3\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME, workspace=WORKSPACE, public_within_org=True)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta.dataset import S3\n",
    "\n",
    "dataset = client.set_dataset(name=\"Census Income\", workspace=WORKSPACE, public_within_org=True)\n",
    "def create_dataset_version(url):\n",
    "    desc = url.strip(\"/\").split(\"/\")[-1]\n",
    "    _, date_range, customer_name = desc.split(\"-\")\n",
    "    tags = \"customer:\" + customer_name\n",
    "    start_date, end_date = date_range.split(\"_\")\n",
    "\n",
    "    version = dataset.create_version(\n",
    "        S3(url), desc=desc, tags=tags,\n",
    "        attrs={\"start_date\": start_date, \"end_date\": end_date},\n",
    "    )\n",
    "    \n",
    "# expected naming convention is s3 base path - <mmddyyyy::mmddyyyy> - <customername>\n",
    "create_dataset_version(\"s3://verta-starter/Demo-01012021_01022021-acme/\")\n",
    "create_dataset_version(\"s3://verta-starter/Demo-01012021_01022021-abc/\")\n",
    "create_dataset_version(\"s3://verta-starter/Demo-01022021_01032021-acme/\")\n",
    "create_dataset_version(\"s3://verta-starter/Demo-01022021_01032021-abc/\")\n",
    "create_dataset_version(\"s3://verta-starter/Demo-01032021_01042021-acme/\")\n",
    "create_dataset_version(\"s3://verta-starter/Demo-01032021_01042021-abc/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files for specific customers and date range\n",
    "def get_dataset_versions_for_customer_daterange(customer, start_date, end_date):\n",
    "    # filter dataset versions by customer + date range\n",
    "    customer_versions = dataset.versions.find(\"tags == customer:\" + customer)\n",
    "    filtered_versions = filter(\n",
    "        lambda datasetv: datetime.strptime(datasetv.get_attribute(\"start_date\"), \"%m%d%Y\") >= start_date\n",
    "                     and datetime.strptime(datasetv.get_attribute(\"end_date\"), \"%m%d%Y\") <= end_date,\n",
    "        customer_versions\n",
    "    )\n",
    "    \n",
    "    return list(filtered_versions)\n",
    "\n",
    "\n",
    "start_date = datetime(2021, 1, 3)\n",
    "end_date   = datetime(2021, 1, 4)\n",
    "customers = [\"acme\", \"abc\"]\n",
    "final_tags = \"customer:\" + \";\".join(customers)\n",
    "final_filtered_versions = []\n",
    "for customer in customers: \n",
    "    final_filtered_versions.extend(get_dataset_versions_for_customer_daterange(customer, start_date, end_date))\n",
    "\n",
    "# merge content\n",
    "final_content = None\n",
    "for datasetv in final_filtered_versions:\n",
    "    if final_content is None:\n",
    "        final_content = datasetv.get_content()\n",
    "    else:\n",
    "        final_content += datasetv.get_content()\n",
    "print(final_content)\n",
    "\n",
    "# create new dataset version using merged content\n",
    "final_version = dataset.create_version(\n",
    "    final_content, tags=final_tags,\n",
    "    attrs={\"start_date\": start_date.strftime(\"%m%d%Y\"), \"end_date\": end_date.strftime(\"%m%d%Y\")},\n",
    ")\n",
    "final_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./\"\n",
    "\n",
    "def download_dataset(s3_url, local_path):\n",
    "    s3_url = s3_url.replace(\"s3://\", \"http://s3.amazonaws.com/\")\n",
    "    print(s3_url)\n",
    "    if not os.path.isfile(local_path):\n",
    "        wget.download(s3_url)\n",
    "\n",
    "train_urls = []\n",
    "test_urls = []\n",
    "for s3_url in final_version.get_content().list_paths():\n",
    "    if os.path.basename(s3_url).startswith(\"census-train\"):\n",
    "        train_urls.append(s3_url)\n",
    "    elif os.path.basename(s3_url).startswith(\"census-test\"):\n",
    "        test_urls.append(s3_url)\n",
    "\n",
    "local_train_paths = []\n",
    "local_test_paths = []\n",
    "for train_url, test_url in zip(train_urls, test_urls):\n",
    "    train_local_path = DATASET_PATH + os.path.basename(train_url)\n",
    "    test_local_path = DATASET_PATH + os.path.basename(test_url)\n",
    "    \n",
    "    download_dataset(train_url, train_local_path)\n",
    "    download_dataset(test_url, test_local_path)\n",
    "    \n",
    "    local_train_paths.append(train_local_path)\n",
    "    local_test_paths.append(test_local_path)\n",
    "\n",
    "# Merge train and test datasets\n",
    "merged_train_csv = pd.concat([pd.read_csv(f) for f in local_train_paths])\n",
    "merged_train_csv.to_csv(\"./merged_train.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "merged_test_csv = pd.concat([pd.read_csv(f) for f in local_test_paths])\n",
    "merged_test_csv.to_csv(\"./merged_test.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./merged_train.csv\")\n",
    "X_train = df_train.iloc[:,:-1]\n",
    "Y_train = df_train.iloc[:, -1]\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "hyperparam_candidates = {\n",
    "    'C': [1e-6, 1e-4],\n",
    "    'solver': ['lbfgs'],\n",
    "    'max_iter': [15, 28],\n",
    "}\n",
    "hyperparam_sets = [dict(zip(hyperparam_candidates.keys(), values))\n",
    "                   for values\n",
    "                   in itertools.product(*hyperparam_candidates.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(hyperparams):\n",
    "    \n",
    "    # create object to track experiment run\n",
    "    run = client.set_experiment_run()\n",
    "    \n",
    "    # create validation split\n",
    "    (X_val_train, X_val_test,\n",
    "     Y_val_train, Y_val_test) = model_selection.train_test_split(X_train, Y_train,\n",
    "                                                                 test_size=0.2,\n",
    "                                                                 shuffle=True)\n",
    "\n",
    "    # log hyperparameters\n",
    "    run.log_hyperparameters(hyperparams)\n",
    "    print(hyperparams, end=' ')\n",
    "    \n",
    "    # create and train model\n",
    "    model = linear_model.LogisticRegression(**hyperparams)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # calculate and log validation accuracy\n",
    "    val_acc = model.score(X_val_test, Y_val_test)\n",
    "    run.log_metric(\"val_acc\", val_acc)\n",
    "    print(\"Validation accuracy: {:.4f}\".format(val_acc))\n",
    "    \n",
    "    # create deployment artifacts\n",
    "    model_api = ModelAPI(X_train, model.predict(X_train))\n",
    "    requirements = [\"scikit-learn\"]\n",
    "    \n",
    "    # save and log model\n",
    "    run.log_model(model, model_api=model_api, custom_modules=[])\n",
    "    run.log_requirements(requirements)\n",
    "    \n",
    "    # log training data\n",
    "    run.log_dataset_version(\"train\", final_version)\n",
    "    \n",
    "# NOTE: run_experiment() could also be defined in a module, and executed in parallel\n",
    "for hyperparams in hyperparam_sets:\n",
    "    run_experiment(hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisit Workflow\n",
    "This section demonstrates querying and retrieving runs via the Client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Best Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = expt.expt_runs.sort(\"metrics.val_acc\", descending=True)[0]\n",
    "print(\"Validation Accuracy: {:.4f}\".format(best_run.get_metric(\"val_acc\")))\n",
    "\n",
    "best_hyperparams = best_run.get_hyperparameters()\n",
    "print(\"Hyperparameters: {}\".format(best_hyperparams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staging\n",
    "The best-performing model can be staged as a registered model, for use downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model = client.get_or_create_registered_model(name=\"Census\", workspace=WORKSPACE, public_within_org=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model.create_version_from_run(best_run.id, name=\"v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy models\n",
    "This registered model version can be deployed to an endpoint, whereupon predictions can be made via a REST endpoint or through the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "registered_model = client.get_registered_model(name=\"Census\", workspace=WORKSPACE)\n",
    "model_version = registered_model.get_version(name=\"v0\")\n",
    "print(model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and update an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = client.get_or_create_endpoint(path=\"/Census\", workspace=WORKSPACE, public_within_org=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.update(model_version, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare live data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"./merged_test.csv\")\n",
    "X_test = df_test.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deployed_model = endpoint.get_deployed_model()\n",
    "\n",
    "for x in itertools.cycle(X_test.values.tolist()):\n",
    "    print(deployed_model.predict([x]))\n",
    "    time.sleep(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
