{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Models with Multiple ML Frameworks on Verta\n",
    "\n",
    "Within Verta, a \"Model\" can be any arbitrary function: a traditional ML model (e.g., sklearn, PyTorch, TF, etc); a function (e.g., squaring a number, making a DB function etc.); or a mixture of the above (e.g., pre-processing code, a DB call, and then a model application.) See more [here](https://docs.verta.ai/verta/collaboration/concepts).\n",
    "\n",
    "This notebook provides an example of how to deploy a XGBoost + Scikit-learn model on Verta as a Verta Standard Model by extending [VertaModelBase](https://verta.readthedocs.io/en/master/_autogen/verta.registry.VertaModelBase.html?highlight=VertaModelBase#verta.registry.VertaModelBase). The same pattern can be used to deploy models made up of any number of ML Frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import six\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Verta import and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart your notebook if prompted on Colab\n",
    "try:\n",
    "    import verta\n",
    "except ImportError:\n",
    "    !pip install verta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure credentials are set up, if not, use below\n",
    "# import os\n",
    "# os.environ['VERTA_EMAIL'] = \n",
    "# os.environ['VERTA_DEV_KEY'] = \n",
    "# os.environ['VERTA_HOST'] = \n",
    "\n",
    "from verta import Client\n",
    "import os\n",
    "PROJECT_NAME = \"Census\"\n",
    "EXPERIMENT_NAME = \"sklearn + xgboost\"\n",
    "client = Client(os.environ['VERTA_HOST'])\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import wget\n",
    "except ImportError:\n",
    "    !pip install wget  # you may need pip3\n",
    "    import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_url = \"http://s3.amazonaws.com/verta-starter/census-train.csv\"\n",
    "train_data_filename = wget.detect_filename(train_data_url)\n",
    "if not os.path.isfile(train_data_filename):\n",
    "    wget.download(train_data_url)\n",
    "\n",
    "test_data_url = \"http://s3.amazonaws.com/verta-starter/census-test.csv\"\n",
    "test_data_filename = wget.detect_filename(test_data_url)\n",
    "if not os.path.isfile(test_data_filename):\n",
    "    wget.download(test_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_data_filename)\n",
    "X_train_hpw = df_train.drop(columns=[\"hours-per-week\", \">50k\"]) # predict hours per week\n",
    "y_train_hpw = df_train[\"hours-per-week\"]\n",
    "\n",
    "X_train_income = df_train.drop(columns=[\"hours-per-week\", \">50k\"])\n",
    "y_train_income = df_train[\">50k\"]\n",
    "\n",
    "X_train_income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_candidates = {\n",
    "    'C': [1e-6, 1e-4],\n",
    "    'solver': ['lbfgs'],\n",
    "    'max_iter': [15, 28],\n",
    "}\n",
    "hyperparam_sets = [dict(zip(hyperparam_candidates.keys(), values))\n",
    "                   for values\n",
    "                   in itertools.product(*hyperparam_candidates.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train/test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(hyperparams, X_train, y_train):\n",
    "    # create object to track experiment run\n",
    "    run = client.set_experiment_run()\n",
    "    \n",
    "    # create validation split\n",
    "    (X_val_train, X_val_test,\n",
    "     y_val_train, y_val_test) = model_selection.train_test_split(X_train, y_train,\n",
    "                                                                 test_size=0.2,\n",
    "                                                                 shuffle=True)\n",
    "\n",
    "    # log hyperparameters\n",
    "    run.log_hyperparameters(hyperparams)\n",
    "    print(hyperparams, end=' ')\n",
    "    \n",
    "    # create and train model\n",
    "    model = linear_model.LogisticRegression(**hyperparams)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # calculate and log validation accuracy\n",
    "    val_acc = model.score(X_val_test, y_val_test)\n",
    "    run.log_metric(\"val_acc\", val_acc)\n",
    "    print(\"Validation accuracy: {:.4f}\".format(val_acc))\n",
    "    \n",
    "# NOTE: run_experiment() could also be defined in a module, and executed in parallel\n",
    "for hyperparams in hyperparam_sets:\n",
    "    run_experiment(hyperparams, X_train_hpw, y_train_hpw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = expt.expt_runs.sort(\"metrics.val_acc\", descending=True)[0]\n",
    "print(\"Validation Accuracy: {:.4f}\".format(best_run.get_metric(\"val_acc\")))\n",
    "\n",
    "best_hyperparams = best_run.get_hyperparameters()\n",
    "print(\"Hyperparameters: {}\".format(best_hyperparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(multi_class='auto', **best_hyperparams)\n",
    "model.fit(X_train_hpw, y_train_hpw)\n",
    "train_acc = model.score(X_train_hpw, y_train_hpw)\n",
    "predicted_hpw = model.predict(X_train_hpw)\n",
    "print(\"Training accuracy: {:.4f}\".format(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predicted_hpw, index=X_train_hpw.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_income_prediction = pd.concat([X_train_income, \n",
    "           pd.DataFrame(\n",
    "               predicted_hpw, columns=[\"predicted_hpw\"],\n",
    "               index=X_train_income.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train_income_prediction, label=y_train_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = model_selection.ParameterGrid({\n",
    "    'eta': [0.5, 0.7],\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'num_class': [10],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"XGBoost\"\n",
    "client.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(hyperparams, X_train, y_train):\n",
    "    run = client.set_experiment_run()\n",
    "    \n",
    "    # log hyperparameters\n",
    "    run.log_hyperparameters(hyperparams)\n",
    "    \n",
    "    # run cross validation on hyperparameters\n",
    "    cv_history = xgb.cv(hyperparams, dtrain,\n",
    "                        nfold=5,\n",
    "                        metrics=(\"merror\", \"mlogloss\"))\n",
    "\n",
    "    # log observations from each iteration\n",
    "    for _, iteration in cv_history.iterrows():\n",
    "        for obs, val in iteration.iteritems():\n",
    "            run.log_observation(obs, val)\n",
    "            \n",
    "    # log error from final iteration\n",
    "    final_val_error = iteration['test-merror-mean']\n",
    "    run.log_metric(\"val_error\", final_val_error)\n",
    "    print(\"{} Mean error: {:.4f}\".format(hyperparams, final_val_error))\n",
    "    \n",
    "# NOTE: run_experiment() could also be defined in a module, and executed in parallel\n",
    "for hyperparams in grid:\n",
    "    run_experiment(\n",
    "        hyperparams, X_train_income_prediction.to_numpy(), y_train_income.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_model = xgb.XGBClassifier(**best_hyperparams)\n",
    "income_model.fit(X_train_income_prediction.to_numpy(), y_train_income.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Register Model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model = client.get_or_create_registered_model(\n",
    "    name=\"census\", labels=[\"xgboost\", \"sklearn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpw_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta.registry import VertaModelBase\n",
    "\n",
    "class CensusTwoStep(VertaModelBase):\n",
    "    def __init__(self, artifacts):\n",
    "        import cloudpickle\n",
    "        self.hpw_model = cloudpickle.load(\n",
    "            open(artifacts[\"hpw_model\"], \"rb\"))\n",
    "        self.income_model = cloudpickle.load(\n",
    "            open(artifacts[\"income_model\"], \"rb\"))\n",
    "        \n",
    "    def predict(self, batch_input):\n",
    "        import numpy as np\n",
    "        results = []\n",
    "        for one_input in batch_input:\n",
    "            output = self.hpw_model.predict(one_input)\n",
    "            output = np.concatenate((np.array(one_input), np.reshape(output, (-1,1))), axis=1)\n",
    "            output = self.income_model.predict(output)\n",
    "            results.append(output)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "cloudpickle.dump(income_model, open(\"income_model.pkl\", \"wb\"))\n",
    "cloudpickle.dump(hpw_model, open(\"hpw_model.pkl\", \"wb\"))\n",
    "\n",
    "my_model = CensusTwoStep(\n",
    "    {\n",
    "        \"hpw_model\" : \"hpw_model.pkl\", \n",
    "        \"income_model\" : \"income_model.pkl\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.predict([X_train_hpw.values.tolist()[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta.environment import Python\n",
    "model_version = registered_model.create_standard_model(\n",
    "    model_cls=CensusTwoStep,\n",
    "    environment=Python(requirements=[\"sklearn\", \"xgboost\"]),\n",
    "    artifacts={\n",
    "        \"hpw_model\" : hpw_model,\n",
    "        \"income_model\" : income_model\n",
    "    },\n",
    "    name=\"v6\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy model to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_multiple_endpoint = client.get_or_create_endpoint(\"census-multiple\")\n",
    "census_multiple_endpoint.update(model_version, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model = census_multiple_endpoint.get_deployed_model()\n",
    "deployed_model.predict([X_train_hpw.values.tolist()[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
